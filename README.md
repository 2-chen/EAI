[**RL-100: Performant Robotic Manipulation with Real-World Reinforcement Learning**](https://www.alphaxiv.org/abs/2510.14830) 2025.10

* 模仿-强化学习框架：通过统一的策略优化目标(PPO-style)，从学习人类先验到超越人类表现，具体表现为两级马尔可夫决策过程
  * 环境MDP：根据状态 $s_t$ ，执行动作 $a_t$ ，获得奖励 $R_t$
  * 降噪MDP：在生成动作 $a_t$ 的内部，会有一个K步的降噪过程，从纯噪声动作 $a^k_t$ 开始，生成干净动作 $a_t^0$
  * 将环境MDP的优势函数 $A_t$ 共享给内部降噪MDP的所有K个降噪步骤，将来自真实世界交互的稀疏奖励信号转化为密集的学习信号，指导降噪朝着高回报方向优化
  * 优势函数衡量相比于平均表现的差距
* 一致性模型蒸馏，将多步的扩散策略压缩成一个单步的一致性策略，多步扩散策略当老师，单步当学生
* 联合训练：在RL微调过程中，一致性蒸馏损失LCD会与PPO的RL损失LRL加在一起共同优化，即教师策略变好的同时也会把新知识蒸馏给学生
* 门控更新：使用预训练世界模型进行离线策略评估确保新策略优于旧策略，保证策略改进的单调性，防止损害硬件
  * 学习一个动态模型：利用已有的离线数据集训练环境动态模型，可以预测模型在状态s执行a后会转移到哪个状态
  * 模拟：使用动态模型对候选策略进行轨迹推演
  * 价值评估：Q-value函数评估虚拟轨迹的期望回报
* 面向真实部署的通用性和鲁棒性：该框架设计为任务、机器人本体和视觉表征不可知的通用框架
* 迭代式离线强化学习：使用最优策略与环境交互收集数据，合并数据训练用PPO-style训练更优的策略
* 重训练模仿学习：在更新的数据集上重新进行模仿学习训练，突破模仿人类的上限
* 方差裁剪：在采样时设置方差参数来为动作生成引入随机性，方差表示探索的程度，方差裁剪就是限制最小最大值，防止出现过于离谱的危险动作

[**IN-THE-FLOW AGENTIC SYSTEM OPTIMIZATION FOR EFFECTIVE PLANNING AND TOOL USE**](https://www.alphaxiv.org/abs/2510.05592) 2025.10

* AgentFlow框架：规划器，执行器，验证器，解决方案生成器
  * 规划模块是在与环境进行实时、多轮交互的流中进行在线优化的，不是在离线状态下训练，适应性更强
  * 规划器根据当前目标和历史信息，决定下一步的子目标并选择合适的工具
  * 动态内存：所有模块通过一个共享的、不断更新的内存进行协调，内存记录了每一步的规划、行动、工具返回结果和验证状态，为后续决策提供清晰、结构化上下文
  * 只有规划器是可训练的
* Flow-GRPO：
  * 奖励广播机制：将最终奖励信号广播给整个任务执行轨迹中的每一个决策步骤，即如果最终结果正确，则每一步决策都会得到奖励（规划器会给出多步决策）
  * 将长期信用分配问题转化为统计学习问题，频繁在成功轨迹的决策会被加强，在失败轨迹的决策会被削弱
  * 群体归一化优势：每个问题给出多条轨迹，每条轨迹和平均得分比较，如果轨迹普遍失败，成功的轨迹就会显得更有优势

[**Agent Learning via Early Experience**](https://www.alphaxiv.org/abs/2510.08558) 2025.10

* 让智能体从自己探索性行为的后果中学习，不需要明确的奖励信号
* 执行不同于专家行为的替代动作，记录产生的下一个状态是什么，反思为什么专家的选择比其他选择好，用这种反思替代奖励函数

[**Training-Free Group Relative Policy Optimization**](https://www.alphaxiv.org/abs/2510.08191) 2025.9

* 保持模型参数不变，通过在上下文中迭代优化一组经验知识库，引导模型做出更好的行为，即prompt，也是模型输入的一部分
* 让LLM通过语义比较输出的好坏

[**Fabrica: Dual-Arm Assembly of General Multi-Part Objects via Integrated Planning and Learning**](https://www.alphaxiv.org/abs/2506.05168) 2025.6

* 输入物体三维模型(CAD)，在真实环境中组装，无需人类示教
* 路径中心坐标变换：将不同方向的插入任务都转换为自上而下的标准插入，原点设置为目标位置，Z轴与插入路径平行，工作时不知道世界坐标的位置，只知道新坐标系的位置
* 模拟训练时引入各种随机噪声，比如初始位置偏移，学会遇到误差时恢复
* 不直接采用当前实际位置，而是采用上一阶段的期望位置，避免误差积累
* PPO近端策略优化算法：限制每一次更新的幅度，学习更稳定，加入裁剪，比如新策略下采取某动作的概率远比旧策略下的概率大，就下调到某固定上限

[**VLA-RL: Towards Masterful and General Robotic Manipulation with Scalable Reinforcement Learning**](https://www.alphaxiv.org/abs/2505.18719) 2025.5

[**π0.5: a Vision-Language-Action Model with Open-World Generalization**](https://www.alphaxiv.org/abs/2504.16054) 2025.4

* 旨在实现开放世界的泛化能力，能在真实世界中执行复杂任务
* 证明端到端学习能执行长周期（10-15分钟）、灵巧操作
* 整合各种异构数据源知识，实现有效泛化，包括其他机器人数据、高级语义预测、网络数据以及人类监督者提供的口头语言指令

[**π0: A Vision-Language-Action Flow Model for General Robot Control**](https://www.alphaxiv.org/abs/2410.24164) seminar 2024.11

* 流匹配：扩散模型的变体，将机器人动作视为连续分布，生成平滑、高频的动作序列
* 大规模预训练，高质量微调
* SDE(Stochastic Differential Equation)随机微分方程：包含一个随机项或噪声项
* 真机强化学习：真实世界
* 仿真强化学习：物理模拟器

[**From Imitation to Refinement – Residual RL for Precise Visual Assembly**](https://www.alphaxiv.org/abs/2407.16677v1/) 2024.7

* 

[**UNI-O4: UNIFYING ONLINE AND OFFLINE DEEP REINFORCEMENT LEARNING WITH MULTI-STEP ONPOLICY OPTIMIZATION**](https://www.alphaxiv.org/abs/2311.03351) 2024.3


**阅读论文prompt**

你是一位跨学科教授与顶级期刊审稿人，习惯“快速鸟瞰→结构化深挖→批判性复盘”。你面向博士阶段读者，要求严谨、可核查、可行动。
	
【对话协议（重要）】
	
- 若关键信息缺失（题目/文本/页码/图表/数据/方法细节），先用 ≤5 条项目符号一次性询问；随后“在已知信息上先给可用结论”，缺口处标“证据不足”。
- 你的推理过程保持内化（不输出思维链），只输出结论 + 证据锚点。
- 表格使用规范：只放“指标/数值/关键词/页码”等短内容；禁止把长句放进表格。
- 引用原文≤20字，并在句尾标注来源位置，如 [p.12] 或 [sec.3.2]。无法定位用 [loc?]。
- 全文中文；必要术语保留英文/符号；跨域读者时首次术语给“一句话词汇表”。
	
【输出结构（逐节输出；保留小标题）】
	
0) Executive Overview
	
- 这篇论文研究了什么问题？为何重要（1句通俗总结）。
- 核心想法/装置（1句）。
- 与典型基线/常识方法相比的主要改进点（1句）。
- 主效果与适用边界（1句，含代表性数字或定性收益）。
- 总体判断：是否值得博士生深入阅读/复现（结论 + 12字理由）。
	
1) Background（相关脉络与定位）
	
- 用3–6条要点给出：问题定义、典型评估范式、常见基线/主流理论、历史演进与现状空白。必要术语做“一句话词汇表”。所有事实加来源标注。
	
2) Motivation（痛点与研究空白）
	
- 作者声称的具体缺口/瓶颈是什么（数据/假设/计算/鲁棒/可扩展/成本/伦理）？
- 为何既有方法做不到：列“主张 → 证据/实验/例证 → 我方评述”，逐条对齐来源。
- 若动机仅为“更好指标”，要求指明：受控变量、评测公平性边界；若文中缺失，标“证据不足”。
	
3) Claimed Contributions
	
- C1..Ck：每条用“一句话贡献 + 证据锚点 + 可复用产出（代码/数据/理论命题/工具）”。
- 给每条贡献标注：原创性（高/中/低）、可复用价值（高/中/低），并说明理由（1句）。
	
4) Method（可落地复述）
	
4.1 核心思想（2–3句，不赘述）
4.2 关键假设与前提（用要点列出；说明适用边界与潜在失败条件）
4.3 形式化与目标（变量/目标函数/约束/推断或识别策略；若为定理/命题，列条件与结论）
4.4 算法/流程（用N步要点，必要时给伪代码式行文；若能，补充时间/空间复杂度或成本量级）
4.5 设计权衡（与可行替代A/B/C的权衡理由；若未说明，记为潜在威胁）
4.6 失败模式（作者提及或可推断的失效场景）
	
5) Results（证据、效应大小与鲁棒性）
	
5.1 数据与设置：数据/样本/语料或材料来源、划分/样本量、预处理、超参/硬件/重复次数/统计方法；公开性与可获得性。
5.2 基线与公平性：基线是否强且最新？训练预算/检索资源/样本是否对齐？
5.3 主要结果（推荐用表格，仅放“任务/数据/指标/本方法/最佳基线/绝对与相对增益/方差或置信区间/页码”）。
5.4 消融与归因：哪一模块/假设真正带来提升？是否报告负结果或跨域/跨数据验证？
5.5 鲁棒与外推：分布外、少样本、噪声/干扰、效率（吞吐/延迟/能耗/成本）、安全/伦理边界。
5.6 可复现性：是否公开代码/权重/数据/脚本；给出最小复现方案（3步内概述）。
	
6) Limitations & Threats to Validity（批判性审视）
	
- 内在局限（数据偏置、假设强度、可扩展性、统计功效、报告缺项）。
- 有效性威胁：内部/外部/构造/结论威胁逐条列举（各1句）。
- 伦理与风险（隐私、滥用、安全、社会影响）；若缺少红队/风险评估，直言其缺失。

7) Future Directions（未来可能改进的方向>=5条）

8) For Busy Readers

- 3条要点式takeaway（每条≤20字）；
- “是否建议投入复现？”是/否+一句话理由。

【自我校验】

- 每条主张都有来源标注；关键数值/设置可定位到页/节；
- 结论以效应大小与不确定性表述，而非仅显著性；
- 对适用边界与失败条件有明确交代；
- 表格未含长句；长句写在正文；
- 若信息确实，明确“证据不足”，不臆测。

【风格】

- 学术而简洁；先结论后理由；多用项目符号与短句。
