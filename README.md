[**Training-Free Group Relative Policy Optimization**](https://www.alphaxiv.org/abs/2510.08191) 2025.9

* 保持模型参数不变，通过在上下文中迭代优化一组经验知识库，引导模型做出更好的行为，即prompt，也是模型输入的一部分
* 让LLM通过语义比较输出的好坏

[**Agent Learning via Early Experience**](https://www.alphaxiv.org/abs/2510.08558) 2025.10

* 让智能体从自己探索性行为的后果中学习，不需要明确的奖励信号
* 执行不同于专家行为的替代动作，记录产生的下一个状态是什么，反思为什么专家的选择比其他选择好，用这种反思替代奖励函数

[**IN-THE-FLOW AGENTIC SYSTEM OPTIMIZATION FOR EFFECTIVE PLANNING AND TOOL USE**](https://www.alphaxiv.org/abs/2510.05592) 2025.10

* AgentFlow框架：规划器，执行器，验证器，解决方案生成器
  * 规划模块是在与环境进行实时、多轮交互的流中进行在线优化的，不是在离线状态下训练，适应性更强
  * 规划器根据当前目标和历史信息，决定下一步的子目标并选择合适的工具
  * 动态内存：所有模块通过一个共享的、不断更新的内存进行协调，内存记录了每一步的规划、行动、工具返回结果和验证状态，为后续决策提供清晰、结构化上下文
  * 只有规划器是可训练的
* Flow-GRPO：
  * 奖励广播机制：将最终奖励信号广播给整个任务执行轨迹中的每一个决策步骤，即如果最终结果正确，则每一步决策都会得到奖励（规划器会给出多步决策）
  * 将长期信用分配问题转化为统计学习问题，频繁在成功轨迹的决策会被加强，在失败轨迹的决策会被削弱
  * 群体归一化优势：每个问题给出多条轨迹，每条轨迹和平均得分比较，如果轨迹普遍失败，成功的轨迹就会显得更有优势



