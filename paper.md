[**OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction**](https://www.alphaxiv.org/abs/2509.26633) seminar 2025.9

* OmniRetarget：将人类的动作演示高效地转换为机器人可学习的运动轨迹
* 不仅转换动作，保留了机器人与物体、环境之间的空间和接触关系，如人类推箱子，转换后要保证机器人的手接触并推动了箱子
* 硬约束保证生成动作的物理可行性，比如杜绝穿模、脚部滑动等问题
* 硬约束条件：无碰撞，关节限制，足部防滑
* 数据增强：从一次人类演示中，生成大量训练数据，如基于一次捡箱子演示，生成捡起不同大小、不同初始位置的箱子的动作
* 由于数据质量高，可以用极简的强化学习方案训练策略，且策略在模拟环境训练好后，可以直接部署到真实的人形机器人上
* SMP(Skinned Multi-Person Linear Model)：参数化的三维人体模型
  * 形状参数：静态体型，高矮胖瘦
  * 姿态参数：动态姿势，关节旋转角
* 交互网格：构建包含机器人关键点、交互物体表面采样点、以及环境表面采样点的四面体网格
* 最小化拉普拉斯形变：用拉普拉斯坐标描述网格中每个点相对于其邻居的局部关系，最小化机器人网格和人类网格之间拉普拉斯坐标的差异
* PHC(Perpetual Humanoid Control)算法：优化整个动作序列，找到能最小化关键节点位置误差的完整关节角轨迹
* GMR算法：逐帧优化，同时匹配关键点的位置和朝向
* VideoMimic算法：保持关键点的相对关系，保持身体各部分的结构，不强行匹配世界坐标系的位置
* IMMA算法：先找到机器人关键点应该处于的理想位置，再根据理想位置求解机器人的实际姿态
  * 网格变形优化：最小化交互网格从人类到机器人的拉普拉斯形变
  * 逆运动学求解：根据理想关键点位置求解一个标准IK问题，找到匹配的关节角度
* OmniRetarget算法：逐帧求解带硬约束的优化问题，最小化交互网格的形变

[**Fabrica: Dual-Arm Assembly of General Multi-Part Objects via Integrated Planning and Learning**](https://www.alphaxiv.org/abs/2506.05168) 2025.6

* 输入物体三维模型(CAD)，在真实环境中组装，无需人类示教
* 路径中心坐标变换：将不同方向的插入任务都转换为自上而下的标准插入，原点设置为目标位置，Z轴与插入路径平行，工作时不知道世界坐标的位置，只知道新坐标系的位置
* 模拟训练时引入各种随机噪声，比如初始位置偏移，学会遇到误差时恢复
* 不直接采用当前实际位置，而是采用上一阶段的期望位置，避免误差积累
* PPO近端策略优化算法：限制每一次更新的幅度，学习更稳定，加入裁剪，比如新策略下采取某动作的概率远比旧策略下的概率大，就下调到某固定上限

[**π0: A Vision-Language-Action Flow Model for General Robot Control**](https://www.alphaxiv.org/abs/2410.24164) seminar 2024.11

* 流匹配：扩散模型的变体，将机器人动作视为连续分布，生成平滑、高频的动作序列
* 大规模预训练，高质量微调
* SDE和PDE:
* 真机强化学习：
* 仿真强化学习：
