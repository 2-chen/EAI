# 模型预测控制(Model Predictive Control, MPC)
每个控制周期，MPC 通过求解优化问题来确定控制输入,同时满足输入和输出的约束条件。

**模型预测控制（MPC）最突出的优点是其处理约束的灵活性，这使其成为解决复杂工业过程中多变量控制问题的最有效方法。**

### 1. 什么是模型预测控制（MPC）？

*   **使用模型 (Model):** MPC的核心是拥有一个过程的数学模型（例如，状态空间模型或阶跃响应模型）。这个模型能够预测系统未来的动态行为。
*   **预测未来 (Predictive):** 在每个控制时刻，MPC利用这个模型来预测系统在未来一段时间内的响应。
*   **优化控制 (Control):** MPC通过求解一个在线优化问题，计算出一系列未来的控制动作（例如，阀门开度变化），其目标是使系统未来的输出尽可能地接近期望值（设定点），同时满足各种约束条件。
*   **滚动优化 (Receding Horizon):** 虽然它计算出了一连串的未来控制动作，但它**只执行第一个**。在下一个时刻，它会获取新的测量值，并重新进行上述的“预测-优化”过程。不断“重新规划”的机制使得MPC能够应对干扰和模型不确定性。

### 2. 为什么MPC如此重要？

*   **强大的约束处理能力：** 在实际工业生产中，系统总是受到各种约束，比如阀门不能开到100%以上、产品的某个指标必须在某个范围内等。传统的控制方法处理这些约束通常很困难且效果不佳，而MPC可以将这些约束作为优化问题的“边界条件”直接、系统地处理。
*   **天然适用于多变量系统 (MIMO)：** 现代工业过程往往是“牵一发而动全身”的，即一个控制输入会影响多个输出变量。MPC的框架天然就是为多变量系统设计的，能够协调地控制所有输入，以达到全局最优的效果。
*   **处理复杂动态：** 对于具有大时滞、非最小相位等复杂动态特性的系统，MPC也能够通过其预测机制进行有效控制。

### 3. MPC的主要算法与理论框架

*   **动态矩阵控制 (DMC, Dynamic Matrix Control):** 使用过程的阶跃响应模型。
*   **模型算法控制 (MAC, Model Algorithmic Control):** 使用过程的脉冲响应模型。
*   **与内模控制 (IMC) 的关系：** 在没有约束的情况下，MPC的结构等价于内模控制（IMC）。将MPC这个看似独立的优化方法与经典的控制理论联系起来，可以运用IMC的成熟理论工具来分析MPC的稳定性与鲁棒性。
*   **与线性二次型控制 (LQC) 的比较：** 无约束的MPC与LQC在数学上是紧密相关的。MPC的真正优势在于当**约束**起作用时，这是传统LQC难以处理的。

# 欠驱动机器人

# 大语言模型在机器人学中的应用

### 主要解决的问题

自动驾驶系统越来越多地利用大型语言模型来理解复杂的驾驶环境并与乘客互动。然而，这带来了一些严重的问题：

1.  **数据安全和隐私风险**：为了让LLM做出决策，车辆需要将大量敏感数据（如精确位置、摄像头图像、雷达点云数据和个人信息）上传到云端的LLM进行处理。
2.  **LLM的不可靠性**：LLMs本身存在一些固有缺陷，比如可能会产生“幻觉”、存在偏见或被恶意输入（“提示注入攻击”）所欺骗。
3.  **对齐问题**：LLM的决策输出可能不完全符合当地的交通法规或人类的价值观。

研究人员提出了一个新颖的、基于多智能体的**安全框架**。这个框架的核心思想是在车辆和云端LLM之间建立一个“安全护栏”。

对比不安全的框架和新框架：

*   **(a) 不安全的框架**：车辆直接将所有数据打包发送给LLM，然后直接执行LLM的指令。这种方式简单直接，但风险极高。
*   **(b) 论文提出的安全框架**：在这个新框架中，数据和指令的流动被几个关键的智能体所管理和审查：
    *   **数据管理器 (Data Manager)**：负责过滤和审查车辆收集到的敏感数据，只将必要且安全的信息发送给LLM。
    *   **提示管理器 (Prompt Manager)**：管理发送给LLM的查询指令，防止恶意或不当的输入。
    *   **评估器 (Evaluator)**：对LLM返回的决策进行审查和验证，确保其符合安全标准、交通法规和人类价值观，然后才将安全的指令传递给车辆执行。

研究人利用他们提出的框架**评估了当前已有的11种主流的LLM自动驾驶方法**（例如Agent-Driver, DriveGPT4, DLAH等）。他们从以下几个维度进行了分析：

*   **驾驶安全率**：模型的决策有多安全。
*   **敏感数据使用量**：模型在系统提示（system prompt）中使用了多少敏感数据。
*   **Token成本**：模型的计算效率如何，Token数量越少越高效。
*   **对齐度排名**：模型的驾驶习惯与人类驾驶员的相似程度。

*   **性能与安全之间的权衡**：研究发现，不同的模型在各项指标上表现各异，存在明显的权衡。例如，“Agent-Driver”模型在安全性上得分最高（99.13%），但其对齐度和数据使用量也较高。而“WayveDriver”模型的效率最高（Token成本最低），但安全性得分相对较低。
*   **GPT-4模型的平衡性**：采用GPT-4作为核心的“SurrealDriver”和“DriveLLM”在安全性和效率之间取得了较好的平衡。
*   **感知能力评估**：研究人员还使用nuScenes-QA数据集测试了这些模型对驾驶环境的感知和理解能力。结果表明，大多数模型在回答“存在性”（比如“有没有行人？”）和“状态”这类简单问题时表现较好，但在需要“计数”或“比较”等更复杂推理的任务上表现较差。

# VLA模型

### 核心思想：什么是 VLA 模型？

VLA 模型是一种能够**接收视觉和语言输入，并据此生成具体动作**的AI模型。

*   **视觉 (Vision):** 模型通过摄像头等传感器观察物理世界，理解物体的状态、位置和环境布局。
*   **语言 (Language):** 模型能理解人类用自然语言下达的指令，例如“请帮我把桌上的红苹果拿过来”。
*   **动作 (Action):** 模型能控制机器人等物理实体，规划并执行一系列动作来完成指令，例如移动手臂、抓取苹果、递给用户。

VLA 模型的目标是让AI代理能够像人一样，通过观察和沟通，在真实世界中执行复杂的任务。

VLA 模型的研究分为了三个主要方向：

1.  **关键组件:** 
    *   **预训练视觉表示 (Pretrained Visual Representations):** 如何训练出更强大的“眼睛”，让模型更好地理解图像和视频内容。
    *   **动力学学习 (Dynamics Learning):** 让模型理解物理规律，比如一个物体被推动后会如何运动。
    *   **世界模型 (World Models):** 让模型在“脑海”中对世界进行模拟和推演，从而进行更长远的规划。

2.  **底层控制策略 (Low-level Control Policies):** 模型的“肌肉记忆”和执行能力。这类模型接收具体的指令（如“拿起杯子”），并直接输出机器人手臂的精确动作序列。不同的实现架构：
    *   基于 Transformer 的模型。
    *   基于扩散模型 (Diffusion Models) 的策略。
    *   以及最新的基于大语言模型 (LLM-based) 的控制策略。

3.  **高层任务规划器 (High-level Task Planners):** 这部分关注的是模型的“大脑”或规划能力。当面对一个复杂、长期的任务时（如“打扫整个房间”），任务规划器负责将其分解成一系列简单的子任务。然后，这些简单的子任务再交由底层的控制策略去执行。

*   **分层架构是主流：** 目前大多数先进的机器人系统都采用“高层规划+底层控制”的分层架构。高层利用大语言模型强大的推理能力进行任务分解，底层则专注于精确、快速地执行动作。
*   **大模型驱动：** VLA 模型的发展深度受益于大语言模型和视觉基础模型的进步。如何将这些大模型的能力迁移到机器人控制上，是当前的研究热点。
*   **数据是瓶颈：** 与互联网上的文本和图像数据相比，高质量的机器人交互数据（即带有动作标签的视觉和语言数据）非常稀缺且难以获取。因此，**数据集、模拟器和基准测试**的建设至关重要。

该领域面临的主要挑战和未来的研究方向：

*   **挑战:**
    *   **模拟到现实的鸿沟 (Sim-to-Real Gap):** 在模拟器中训练好的模型，在真实机器人上表现往往会打折扣。
    *   **泛化能力:** 如何让模型能够处理在训练中从未见过的物体、指令和环境。
*   **未来方向:**
    *   **机器人基础模型 (Foundation Models for Robotics):** 研发一个能适用于多种不同机器人和任务的通用基础模型。
    *   **更丰富的多模态融合 (Richer Multi-modality):** 除了视觉和语言，未来还可能融合触觉、听觉等更多模态的信息。

# 三维视觉

3D生成分为四个主要的算法范式：

*   **前馈生成（Feedforward Generation）：** 直接使用生成模型（如生成对抗网络 (GANs)、扩散模型、自回归模型、变分自编码器 (VAEs) 和归一化流）来生成3D表示。
*   **基于优化的生成（Optimization-Based Generation）：** 这些方法通过运行时优化来生成3D模型，通常利用预训练的多模态网络，根据用户指定的提示（例如，文本到3D或图像到3D）优化3D模型。
*   **程序生成（Procedural Generation）：** 这涉及使用一系列规则（如分形几何、L-系统、噪声函数和元胞自动机）来创建3D模型和纹理。
*   **生成式新视图合成（Generative Novel View Synthesis）：** 这类方法侧重于从单个输入图像预测新视图，通常通过调节3D信息来使用3D感知方法，而不明确利用3D表示来强制执行3D一致性。

### 人类3维视觉
人类的3维视觉主要通过双眼视差实现。此外，人类还利用以下线索感知3维空间：
- **运动视差**：当头部或物体移动时，近处物体比远处物体移动得更快。
- **遮挡**：近物体会遮挡远物体。
- **透视**：平行线会在远处汇聚。
- **阴影和光照**：光影变化暗示物体的形状和深度。

### 机器3维视觉
在计算机和机器人领域，3维视觉是通过传感器和算法实现的，常见技术包括：
- **立体视觉（Stereo Vision）**：使用两个摄像头模拟人眼，通过图像差异计算深度。
- **结构光（Structured Light）**：投射特定光图案（如红外条纹）到物体上，通过图案变形计算3维形状。
- **激光雷达（LiDAR）**：发射激光并测量反射时间，直接获取高精度的3维点云数据。
- **时间飞行（ToF）**：通过测量光从发射到反射回来的时间计算距离。

### 挑战
- **光照变化**：强光、弱光或反光表面会影响传感器性能。
- **遮挡问题**：物体被部分遮挡时，难以完整重建。
- **计算复杂度**：实时处理大量3维数据需要强大的计算能力。

# 四维视觉

利用大语言模型进行视频理解（Video Understanding）的最新研究进展。这类模型通常被称为 **Vid-LLMs**。

**第一个维度：根据视频信息的处理方式，分为三大框架：**

1.  **视频分析器 × LLM (Video Analyzer × LLM)**：
    *   **工作原理**：首先使用独立的“视频分析器”模块（如视频字幕生成模型、目标检测模型、语音识别模型等）将视频内容转换成纯文本描述，然后将文本信息被输入给LLM进行后续的推理和问答。
    *   **特点**：LLM不直接“看”视频，而是“阅读”关于视频的报告。这种方法实现简单，可以利用现成的强大LLM，但可能会丢失视频中的细节信息。

2.  **视频嵌入器 × LLM (Video Embedder × LLM)**：
    *   **工作原理**：这种框架使用一个“视频嵌入器”（通常是视觉编码器，如ViT或CLIP）将视频帧转换成一系列数字向量。然后通过一个适配器（Adapter）将这些视觉向量对齐并输入给LLM。
    *   **特点**：LLM直接处理来自视频的视觉特征，能够更好地捕捉视觉细节。这是目前最主流的研究方向，但需要对模型进行端到端的训练或微调。

3.  **（分析器 + 嵌入器）× LLM (Analyzer + Embedder) × LLM**：
    *   **工作原理**：这是一个混合框架，同时为LLM提供来自视频的文本描述和视觉向量。
    *   **特点**：结合了前两种方法的优点，理论上能提供最全面的信息，但结构也最复杂。

**第二个维度：根据LLM在系统中的功能，分为五种角色：**

*   **LLM作为总结器 (Summarizer)**：LLM接收处理后的视频信息，并根据指令生成摘要、回答问题。信息流是单向的。
*   **LLM作为管理器 (Manager)**：LLM扮演一个“指挥官”的角色，可以主动调用不同的工具（视频分析器）来获取信息，并与工具进行多轮交互来解决复杂问题。
*   **LLM作为文本解码器 (Text Decoder)**：这是最常见的角色，LLM将输入的视频向量解码为自然语言文本输出。
*   **LLM作为回归器 (Regressor)**：除了生成文本，LLM还能预测连续值，例如视频中某个事件发生的时间戳或物体的位置坐标。
*   **LLM作为隐藏层 (Hidden Layer)**：LLM的输出不直接作为最终结果，而是作为中间特征，再送入一个专门的任务头（task-specific head）来完成特定任务（如定位）。

### 3. 视频理解技术的发展脉络

1.  **传统方法**：依赖于手工设计的特征，如SIFT、HOG等。
2.  **早期神经网络模型**：使用CNN和RNN（如LSTM）来处理视频，例如双流网络（Two-stream Networks）。
3.  **自监督视频预训练 (Self-supervised Video Pretraining)**：通过自监督学习（如VideoBERT、VideoMAE）在大规模无标签视频数据上预训练模型，使其具备更强的泛化能力。
4.  **用于视频理解的大语言模型 (Vid-LLMs)**：将强大的LLM引入，使模型不仅能识别内容，更能进行复杂的推理、交互和开放式回答，标志着视频理解进入了一个新的范式。

### 4. 任务、基准和评估

论文详细梳理了视频理解领域的各种任务，并将其分为三类：
*   **抽象理解任务 (Abstract Understanding)**：如视频分类、视频字幕生成、文本-视频检索等。
*   **时序理解任务 (Temporal Understanding)**：如时序动作定位、视频摘要、事件边界检测等。
*   **时空理解任务 (Spatiotemporal Understanding)**：如目标跟踪、视频对象分割、时空定位等。

**未来挑战**：
1.  **更细粒度的理解**：如何理解更细微的动作、情感和场景动态。
2.  **长视频理解**：如何高效处理数小时的长视频并保持对关键信息的记忆。
3.  **多模态融合**：如何更好地融合视频中的视觉、音频、文本等多模态信息。
4.  **模型幻觉 (Hallucination)**：如何减少模型“凭空捏造”不符合视频事实的内容。

# 视觉提示

# 可供性锚定

1. 可供性锚定的形式化定义与评测指标；  
2. 基于大模型/VLM 的 2-D/3-D 锚定网络结构；  
3. 锚定误差对机器人抓取或操作成功率的实证分析。  

在具身智能文献中，“可供性锚定（affordance anchoring）”指的是：  
把抽象的动作语义（“可以抓”“可以坐”或自然语言指令）精确映射到感知信号中的**具体空间-物理单元**——例如图像像素、2-D/3-D 关键点、点云片或物体部件——从而让“可供性”不再是对象级标签，而成为**可坐标化、可度量、可执行**的机器人接口。简言之，它回答“可以怎么做”之后，还要回答“在哪儿做、以什么姿态做”。

1. **可供性（Affordance）**  
   环境-物体相对于**特定智能体**提供的潜在交互方式；同一物体对不同本体（人 vs 双指夹爪 vs 五指手）具有不同可供性。

2. **锚定（Anchoring / Grounding）**  
   在实时感知流中**持续保持**“语义 ⇄ 物理位姿”对应关系，使高层符号能随物体移动、遮挡、变形而更新，而不丢失身份与意义。

3. **具身智能中的闭环**  
   感知 → 锚定可供性区域 → 生成动作/轨迹 → 执行并观测结果 → 更新锚定；利用身体反馈不断修正“哪里可交互”。

### 锚定到哪一层？——主流表征粒度

| 粒度 | 输出形式 | 典型方法 | 适用场景 |
|---|---|---|---|
| 像素级 | 2-D 热力图 / 掩膜 | AffordanceLLM, Cross-View-AG | 平面抓取、推拨 |
| 关键点 | 2-D/3-D 点坐标 | KITE, Robo-ABC | 开抽屉、按按钮 |
| 区域-部件 | 3-D 部件分割 + 向量/面片 | CoPa, SceneFun3D | 家具拆装、工具使用 |
| 语义-技能 | 可执行原语索引 | VoxPoser, Affordance Functions | 语言指令→技能序列 |

### 算法流程（以视觉-语言输入为例）

1. **多模态编码**  
   视觉流：RGB-D、点云 → 视觉骨干(ViT, PointNet++)  
   语言流：指令或标签 → 大语言模型 / CLIP 文本编码器

2. **跨模态对齐**  
   使用对比学习或注意力把“可抓”等文本嵌入与空间特征做相似度匹配，生成**密集相似度张量**。

3. **锚定解码**  
   上采样到与原图/点云相同分辨率 → 经过 Spatial- softmax 得到 2-D/3-D 概率图；取峰值或采样即得坐标+法向+接触力方向。

4. **时序滤波 / 身份维持**  
   引入轻量级跟踪（Kalman 或粒子滤波）或对象级 ID，使同一物体在遮挡再出现后锚定不漂移。

### 四、与“可供性检测”或“grounding”区别

| 维度 | 传统检测 | 可供性锚定 |
|---|---|---|
| 输出 | 类别或 bbox | 精确 6-DoF 接触姿态 |
| 监督 | 强标签（人工框） | 弱/零样本，可自监督 |
| 时序 | 单帧 | 多帧一致性，需跟踪 |
| 闭环 | 开环感知 | 与动作、力控耦合 |

### 六、前沿趋势

1. **大模型即锚定器**  
   利用 VLM 的丰富先验实现“一句话”零样本锚定。
2. **机器人-感知- aware**  
   把夹爪几何、工作空间限界直接注入网络，输出“对本机器人可执行”的专属可供性。
3. **4-D 时空锚定**  
   在动态场景里同时锚定“何时 + 何地”可交互（例如摆动中的锤子柄），结合时序扩散策略做预测。
4. **语言→价值图→控制**  
   VoxPoser 等直接生成 3-D 价值场，跳过显式坐标，一步完成锚定与运动规划。

# 机器人巡航

