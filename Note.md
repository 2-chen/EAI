# 模型预测控制(Model Predictive Control, MPC)
每个控制周期，MPC 通过求解优化问题来确定控制输入,同时满足输入和输出的约束条件。

**模型预测控制（MPC）最突出的优点是其处理约束的灵活性，这使其成为解决复杂工业过程中多变量控制问题的最有效方法。**

### 1. 什么是模型预测控制（MPC）？
与传统的控制器（如PID）不同，MPC的工作方式更像一个“深思熟虑的规划者”：

*   **使用模型 (Model):** MPC的核心是拥有一个过程的数学模型（例如，状态空间模型或阶跃响应模型）。这个模型能够预测系统未来的动态行为。
*   **预测未来 (Predictive):** 在每个控制时刻，MPC利用这个模型来预测系统在未来一段时间（称为“预测时域”）内的响应。
*   **优化控制 (Control):** MPC通过求解一个在线优化问题，来计算出一系列未来的控制动作（例如，阀门开度变化），其目标是使系统未来的输出尽可能地接近期望值（设定点），同时满足各种约束条件。
*   **滚动优化 (Receding Horizon):** 这是MPC最巧妙的一点。虽然它计算出了一连串的未来控制动作，但它**只执行第一个**。然后，在下一个时刻，它会获取新的测量值，并重新进行上述的“预测-优化”过程。这种不断“重新规划”的机制使得MPC能够应对干扰和模型不确定性，具有很强的鲁棒性。

### 2. 为什么MPC如此重要？

*   **强大的约束处理能力：** 这是论文反复强调的MPC最大优势。在实际工业生产中，系统总是受到各种约束，比如阀门不能开到100%以上、反应釜的温度不能超过安全上限、产品的某个指标必须在某个范围内等。传统的控制方法处理这些约束通常很困难且效果不佳，而MPC可以将这些约束作为优化问题的“边界条件”直接、系统地处理，确保生产过程既高效又安全。
*   **天然适用于多变量系统 (MIMO)：** 现代工业过程往往是“牵一发而动全身”的，即一个控制输入会影响多个输出变量。MPC的框架天然就是为多变量系统设计的，能够协调地控制所有输入，以达到全局最优的效果。
*   **处理复杂动态：** 对于具有大时滞、非最小相位等复杂动态特性的系统，MPC也能够通过其预测机制进行有效控制。

### 3. MPC的主要算法与理论框架

*   **动态矩阵控制 (DMC, Dynamic Matrix Control):** 使用过程的阶跃响应模型，是当时工业界应用最成功的MPC算法之一。
*   **模型算法控制 (MAC, Model Algorithmic Control):** 使用过程的脉冲响应模型。
*   **与内模控制 (IMC) 的关系：** **在没有约束的情况下，MPC的结构等价于内模控制（IMC）**。将MPC这个看似独立的优化方法与经典的控制理论联系起来，可以运用IMC的成熟理论工具来分析MPC的稳定性与鲁棒性。
*   **与线性二次型控制 (LQC) 的比较：** 无约束的MPC与LQC在数学上是紧密相关的。MPC的真正优势在于当**约束**起作用时，这是传统LQC难以处理的。

### 4. 论文讨论的延伸问题与挑战

*   **不同目标函数范数：** 讨论了在优化问题中使用2-范数（二次规划QP）、1-范数（线性规划LP）或∞-范数（线性规划LP）作为目标函数的区别。2-范数使误差平方和最小，而1-范数和∞-范数则更关注误差的峰值。
*   **非线性MPC (NMPC)：** 论文预见性地指出，可以将MPC的思想扩展到非线性系统，通过使用非线性模型进行预测和优化，但这会带来巨大的计算挑战。
*   **鲁棒性：** 尽管MPC的滚动优化机制带来了一定的鲁棒性，但如何系统地设计控制器以应对模型不确定性，仍然是一个重要的研究课题。

### 总结
MPC的**核心价值在于处理约束**，并将其与主流控制理论（如IMC和LQC）建立了深刻的联系。

# 欠驱动机器人

# 大语言模型在机器人学中的应用

# VLA模型

# 三维视觉

3D生成分为四个主要的算法范式：

*   **前馈生成（Feedforward Generation）：** 这种方法直接使用生成模型（如生成对抗网络 (GANs)、扩散模型、自回归模型、变分自编码器 (VAEs) 和归一化流）来生成3D表示。
*   **基于优化的生成（Optimization-Based Generation）：** 这些方法通过运行时优化来生成3D模型，通常利用预训练的多模态网络，根据用户指定的提示（例如，文本到3D或图像到3D）优化3D模型。
*   **程序生成（Procedural Generation）：** 这涉及使用一系列规则（如分形几何、L-系统、噪声函数和元胞自动机）来创建3D模型和纹理。
*   **生成式新视图合成（Generative Novel View Synthesis）：** 这类方法侧重于从单个输入图像预测新视图，通常通过调节3D信息来使用3D感知方法，而不明确利用3D表示来强制执行3D一致性。

神经场景表示（显式、隐式和混合表示）作为3D生成骨干的重要性。这些表示，连同可微分渲染算法，对于创建3D模型和渲染2D图像至关重要。

### 人类3维视觉
人类的3维视觉主要通过双眼视差（binocular disparity）实现。此外，人类还利用以下线索感知3维空间：
- **运动视差**：当头部或物体移动时，近处物体比远处物体移动得更快。
- **遮挡**：近物体会遮挡远物体。
- **透视**：平行线会在远处汇聚。
- **阴影和光照**：光影变化暗示物体的形状和深度。

### 机器3维视觉
在计算机和机器人领域，3维视觉是通过传感器和算法实现的，常见技术包括：
- **立体视觉（Stereo Vision）**：使用两个摄像头模拟人眼，通过图像差异计算深度。
- **结构光（Structured Light）**：投射特定光图案（如红外条纹）到物体上，通过图案变形计算3维形状。
- **激光雷达（LiDAR）**：发射激光并测量反射时间，直接获取高精度的3维点云数据。
- **时间飞行（ToF）**：通过测量光从发射到反射回来的时间计算距离。

### 挑战
尽管3维视觉技术取得了显著进展，但仍面临一些挑战：
- **光照变化**：强光、弱光或反光表面会影响传感器性能。
- **遮挡问题**：物体被部分遮挡时，难以完整重建。
- **计算复杂度**：实时处理大量3维数据需要强大的计算能力。
- **成本**：高精度传感器（如LiDAR）价格昂贵。

# 四维视觉

利用大语言模型进行视频理解（Video Understanding）的最新研究进展。这类模型通常被称为 **Vid-LLMs**。

*   现有的Vid-LLMs是如何工作的？
*   它们可以被分为哪些主要类型？
*   这个领域经历了怎样的发展？
*   如何评估这些模型？
*   未来的研究方向和挑战是什么？

### 2. 核心贡献：Vid-LLMs的分类法（Taxonomy）

将现有的Vid-LLMs进行归类。从两个维度展开

**第一个维度：根据视频信息的处理方式，分为三大框架：**

1.  **视频分析器 × LLM (Video Analyzer × LLM)**：
    *   **工作原理**：首先使用独立的“视频分析器”模块（如视频字幕生成模型、目标检测模型、语音识别模型等）将视频内容转换成纯文本描述，然后将文本信息被输入给LLM进行后续的推理和问答。
    *   **特点**：LLM不直接“看”视频，而是“阅读”关于视频的报告。这种方法实现简单，可以利用现成的强大LLM，但可能会丢失视频中的细节信息。

2.  **视频嵌入器 × LLM (Video Embedder × LLM)**：
    *   **工作原理**：这种框架使用一个“视频嵌入器”（通常是视觉编码器，如ViT或CLIP）将视频帧转换成一系列数字向量。然后通过一个适配器（Adapter）将这些视觉向量对齐并输入给LLM。
    *   **特点**：LLM直接处理来自视频的视觉特征，能够更好地捕捉视觉细节。这是目前最主流的研究方向，但需要对模型进行端到端的训练或微调。

3.  **（分析器 + 嵌入器）× LLM (Analyzer + Embedder) × LLM**：
    *   **工作原理**：这是一个混合框架，同时为LLM提供来自视频的文本描述和视觉向量。
    *   **特点**：结合了前两种方法的优点，理论上能提供最全面的信息，但结构也最复杂。

**第二个维度：根据LLM在系统中的功能，分为五种角色：**

*   **LLM作为总结器 (Summarizer)**：LLM接收处理后的视频信息，并根据指令生成摘要、回答问题。信息流是单向的。
*   **LLM作为管理器 (Manager)**：LLM扮演一个“指挥官”的角色，可以主动调用不同的工具（视频分析器）来获取信息，并与工具进行多轮交互来解决复杂问题。
*   **LLM作为文本解码器 (Text Decoder)**：这是最常见的角色，LLM将输入的视频向量解码为自然语言文本输出。
*   **LLM作为回归器 (Regressor)**：除了生成文本，LLM还能预测连续值，例如视频中某个事件发生的时间戳或物体的位置坐标。
*   **LLM作为隐藏层 (Hidden Layer)**：LLM的输出不直接作为最终结果，而是作为中间特征，再送入一个专门的任务头（task-specific head）来完成特定任务（如定位）。

### 3. 视频理解技术的发展脉络

1.  **传统方法**：依赖于手工设计的特征，如SIFT、HOG等。
2.  **早期神经网络模型**：使用CNN和RNN（如LSTM）来处理视频，例如双流网络（Two-stream Networks）。
3.  **自监督视频预训练 (Self-supervised Video Pretraining)**：通过自监督学习（如VideoBERT、VideoMAE）在大规模无标签视频数据上预训练模型，使其具备更强的泛化能力。
4.  **用于视频理解的大语言模型 (Vid-LLMs)**：将强大的LLM引入，使模型不仅能识别内容，更能进行复杂的推理、交互和开放式回答，标志着视频理解进入了一个新的范式。

### 4. 任务、基准和评估

论文详细梳理了视频理解领域的各种任务，并将其分为三类：
*   **抽象理解任务 (Abstract Understanding)**：如视频分类、视频字幕生成、文本-视频检索等。
*   **时序理解任务 (Temporal Understanding)**：如时序动作定位、视频摘要、事件边界检测等。
*   **时空理解任务 (Spatiotemporal Understanding)**：如目标跟踪、视频对象分割、时空定位等。

**未来挑战**：
1.  **更细粒度的理解**：如何理解更细微的动作、情感和场景动态。
2.  **长视频理解**：如何高效处理数小时的长视频并保持对关键信息的记忆。
3.  **多模态融合**：如何更好地融合视频中的视觉、音频、文本等多模态信息。
4.  **模型幻觉 (Hallucination)**：如何减少模型“凭空捏造”不符合视频事实的内容。

# 视觉提示

# 可供性锚定

重点关注：  
1. 可供性锚定的形式化定义与评测指标；  
2. 基于大模型/VLM 的 2-D/3-D 锚定网络结构；  
3. 锚定误差对机器人抓取或操作成功率的实证分析。  

在具身智能文献中，“可供性锚定（affordance anchoring）”指的是：  
把抽象的动作语义（“可以抓”“可以坐”或自然语言指令）精确映射到感知信号中的**具体空间-物理单元**——例如图像像素、2-D/3-D 关键点、点云片或物体部件——从而让“可供性”不再是对象级标签，而成为**可坐标化、可度量、可执行**的机器人接口。简言之，它回答“可以怎么做”之后，还要回答“在哪儿做、以什么姿态做”。

### 一、概念拆解

1. **可供性（Affordance）**  
   环境-物体相对于**特定智能体**提供的潜在交互方式；同一物体对不同本体（人 vs 双指夹爪 vs 五指手）具有不同可供性。

2. **锚定（Anchoring / Grounding）**  
   在实时感知流中**持续保持**“语义 ⇄ 物理位姿”对应关系，使高层符号能随物体移动、遮挡、变形而更新，而不丢失身份与意义。

3. **具身智能中的闭环**  
   感知 → 锚定可供性区域 → 生成动作/轨迹 → 执行并观测结果 → 更新锚定；利用身体反馈不断修正“哪里可交互”。


### 二、锚定到哪一层？——主流表征粒度

| 粒度 | 输出形式 | 典型方法 | 适用场景 |
|---|---|---|---|
| 像素级 | 2-D 热力图 / 掩膜 | AffordanceLLM, Cross-View-AG | 平面抓取、推拨 |
| 关键点 | 2-D/3-D 点坐标 | KITE, Robo-ABC | 开抽屉、按按钮 |
| 区域-部件 | 3-D 部件分割 + 向量/面片 | CoPa, SceneFun3D | 家具拆装、工具使用 |
| 语义-技能 | 可执行原语索引 | VoxPoser, Affordance Functions | 语言指令→技能序列 |

### 三、算法流程（以视觉-语言输入为例）

1. **多模态编码**  
   视觉流：RGB-D、点云 → 视觉骨干(ViT, PointNet++)  
   语言流：指令或标签 → 大语言模型 / CLIP 文本编码器

2. **跨模态对齐**  
   使用对比学习或注意力把“可抓”等文本嵌入与空间特征做相似度匹配，生成**密集相似度张量**。

3. **锚定解码**  
   上采样到与原图/点云相同分辨率 → 经过 Spatial- softmax 得到 2-D/3-D 概率图；取峰值或采样即得坐标+法向+接触力方向。

4. **时序滤波 / 身份维持**  
   引入轻量级跟踪（Kalman 或粒子滤波）或对象级 ID，使同一物体在遮挡再出现后锚定不漂移。

### 四、与“可供性检测”或“grounding”区别

| 维度 | 传统检测 | 可供性锚定 |
|---|---|---|
| 输出 | 类别或 bbox | 精确 6-DoF 接触姿态 |
| 监督 | 强标签（人工框） | 弱/零样本，可自监督 |
| 时序 | 单帧 | 多帧一致性，需跟踪 |
| 闭环 | 开环感知 | 与动作、力控耦合 |

### 六、前沿趋势

1. **大模型即锚定器**  
   利用 VLM 的丰富先验实现“一句话”零样本锚定；微调仅需 <1% 参数。

2. **机器人-感知- aware**  
   把夹爪几何、工作空间限界直接注入网络，输出“对本机器人可执行”的专属可供性。

3. **4-D 时空锚定**  
   在动态场景里同时锚定“何时 + 何地”可交互（例如摆动中的锤子柄），结合时序扩散策略做预测。

4. **语言→价值图→控制**  
   VoxPoser 等直接生成 3-D 价值场，跳过显式坐标，一步完成锚定与运动规划。

---

### 七、小结

“可供性锚定”是具身智能把**抽象交互语义**转成**可执行物理坐标**的咽喉环节；它让机器人不仅能“知道杯子能抓”，更能实时告诉控制器“在杯把中心、沿 –Z 轴、宽 60 mm 处下爪”。随着多模态大模型与自监督学习的推进，锚定正从 2-D 热力图走向 3-D 部件、再到时空-技能一体化，为机器人在开放环境中完成复杂操作提供厘米级、毫秒级的“语义-动作”接口。

# 机器人巡航

